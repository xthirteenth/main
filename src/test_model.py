import argparse
import torch
import torch.nn as nn
from torchvision import datasets, transforms
from models.cnn import SimpleCNN
from utils import load_config, set_seed, get_device
import matplotlib.pyplot as plt

def test_model(model_path, config_path='config/config.json'):
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = load_config(config_path)
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
    set_seed(config['random_seed'])
    
    # –í—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    device = get_device(config)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞
    test_dataset = datasets.MNIST('./data', train=False, transform=transform)
    test_loader = torch.utils.data.DataLoader(
        test_dataset, 
        batch_size=config['training']['batch_size'], 
        shuffle=False
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = SimpleCNN(
        input_channels=config['model']['input_channels'],
        num_classes=config['model']['num_classes'],
        dropout_rate=config['model']['dropout_rate']
    ).to(device)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–π
    criterion = nn.CrossEntropyLoss()
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print(f"üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_path}")
    
    model.eval()
    total_loss = 0.0
    correct = 0
    total_samples = 0
    
    confusion_matrix = torch.zeros(10, 10, dtype=torch.int32)
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            
            total_samples += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
            for t, p in zip(labels.view(-1), predicted.view(-1)):
                confusion_matrix[t.long(), p.long()] += 1
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    test_loss = total_loss / len(test_loader)
    test_accuracy = 100 * correct / total_samples
    
    print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print(f"   –ü–æ—Ç–µ—Ä—è: {test_loss:.4f}")
    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {test_accuracy:.2f}%")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
    print("\nüî¢ –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
    print(confusion_matrix)
    
    return {
        'loss': test_loss,
        'accuracy': test_accuracy,
        'confusion_matrix': confusion_matrix,
        'model': model,
        'test_loader': test_loader,
        'device': device
    }

def visualize_predictions(model, test_loader, device, num_images=5):
    model.eval()
    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))
    
    with torch.no_grad():
        for i, (images, labels) in enumerate(test_loader):
            if i >= 1:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á
                break
            
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            
            for j in range(min(num_images, images.size(0))):
                img = images[j].cpu().squeeze().numpy()
                axes[j].imshow(img, cmap='gray')
                title = f"–ò—Å—Ç–∏–Ω–∞: {labels[j].item()}\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predicted[j].item()}"
                axes[j].set_title(title)
                axes[j].axis('off')
    
    plt.tight_layout()
    plt.savefig('predictions.png')
    plt.close()

def main():
    parser = argparse.ArgumentParser(description='–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏')
    parser.add_argument('model_path', type=str, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –≤–µ—Å–∞–º–∏ –º–æ–¥–µ–ª–∏')
    parser.add_argument('--config', type=str, default='config/config.json', help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É')
    
    args = parser.parse_args()
    
    # –í—ã–ø–æ–ª–Ω–∏—Ç—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å
    results = test_model(args.model_path, args.config)
    
    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    visualize_predictions(results['model'], results['test_loader'], results['device'])

if __name__ == '__main__':
    main() 